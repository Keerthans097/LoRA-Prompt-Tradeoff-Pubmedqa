{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c838d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -U \"transformers>=4.43\" \"accelerate>=0.33\" \"peft>=0.12.0\" bitsandbytes datasets evaluate scikit-learn sentence-transformers tqdm matplotlib numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf34b32",
   "metadata": {},
   "source": [
    "# Initial Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, math, time, random, gc\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, f1_score\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig, TrainingArguments, Trainer, set_seed\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- project config ---\n",
    "CONFIG = {\n",
    "    # models\n",
    "    \"PROMPT_MODEL\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"LORA_BASE_MODEL\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "\n",
    "    # data (pubmed_qa pqa_labeled)\n",
    "    \"DATASET_NAME\": \"pubmed_qa\",\n",
    "    \"DATASET_CONFIG\": \"pqa_labeled\",\n",
    "\n",
    "    # training switch\n",
    "    #\"USE_QLORA\": True,                # True: train,eval LoRA; False: prompt-only baselines\n",
    "    \"USE_QLORA\": False,\n",
    "\n",
    "    \n",
    "    # qlora hyperparams\n",
    "    \"QLORA_RANKS\": [4],\n",
    "    \"LORA_ALPHA\": 32,\n",
    "    \"LORA_DROPOUT\": 0.1,\n",
    "    \"TARGET_MODULES\": [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"],\n",
    "\n",
    "    # optimization\n",
    "    \"EPOCHS\": 5,\n",
    "    \"LR\": 3e-5,#\n",
    "    \"WARMUP_RATIO\": 0.1,\n",
    "    \"PER_DEVICE_TRAIN_BS\": 1,\n",
    "    \"PER_DEVICE_EVAL_BS\": 1,\n",
    "    \"GRAD_ACCUM_STEPS\": 16,\n",
    "\n",
    "    # sequence lengths\n",
    "    \"MAX_INPUT_TOKENS\": 750,\n",
    "    \"MAX_NEW_TOKENS\": 4,              # classification over {yes,no,maybe}\n",
    "\n",
    "    # prompt strategies (used only when USE_QLORA=False)\n",
    "    \"PROMPT_STYLES\": [\"zero\", \"domain\", \"cot\"],\n",
    "\n",
    "    # system / logging\n",
    "    \"BF16\": True,\n",
    "    \"REPORT_TO\": \"none\",\n",
    "    \"WORK_DIR\": \"./runs_pubmedqa\",\n",
    "    \"SEED\": 42,\n",
    "\n",
    "    # trial caps (None means full split)\n",
    "    \"TRIAL_LIMIT_TRAIN\": None,\n",
    "    \"TRIAL_LIMIT_TEST\": None,\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"WORK_DIR\"], exist_ok=True)\n",
    "\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)  \n",
    "\n",
    "# --- reproducibility ---\n",
    "set_seed(CONFIG[\"SEED\"])\n",
    "random.seed(CONFIG[\"SEED\"])\n",
    "np.random.seed(CONFIG[\"SEED\"])\n",
    "torch.manual_seed(CONFIG[\"SEED\"])\n",
    "\n",
    "# --- device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# --- labels and parsing helpers ---\n",
    "LABELS = [\"yes\", \"no\", \"maybe\"]\n",
    "LABEL2ID = {l: i for i, l in enumerate(LABELS)}\n",
    "ID2LABEL = {i: l for l, i in LABEL2ID.items()}\n",
    "\n",
    "def parse_label(text: str) -> str:\n",
    "    \"\"\"extract one of {yes,no,maybe} from decoded generation text.\"\"\"\n",
    "    t = text.strip().lower()\n",
    "    for lab in LABELS:\n",
    "        if lab in t:\n",
    "            return lab\n",
    "    return \"maybe\"\n",
    "\n",
    "# --- prompt builders ---\n",
    "def build_prompt(question: str, context: str, style: str = \"domain\") -> str:\n",
    "    \"\"\"return a prompt string according to style.\"\"\"\n",
    "    if style == \"zero\":\n",
    "        return (\n",
    "            \"You are a helpful assistant for biomedical QA.\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Context: {context}\\n\"\n",
    "            \"Answer exactly one word: yes, no, or maybe.\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "    if style == \"cot\":\n",
    "        return (\n",
    "            \"You are a biomedical QA assistant. Think step by step.\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Context: {context}\\n\"\n",
    "            \"First reason step-by-step, then on a new line output: Final answer: yes or no or maybe.\\n\"\n",
    "            \"Reasoning:\"\n",
    "        )\n",
    "    # default: domain\n",
    "    return (\n",
    "        \"You are a biomedical QA assistant specialized in PubMed abstracts.\\n\"\n",
    "        \"Use the context to answer exactly one word among: yes, no, maybe.\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Context: {context}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "\n",
    "def safe_float(x, default=0.0):\n",
    "    try:\n",
    "        xf = float(x)\n",
    "        if math.isnan(xf) or math.isinf(xf):\n",
    "            return default\n",
    "        return xf\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def ensure_dir(p: str) -> str:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "print(\"CONFIG and utilities ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514861a3",
   "metadata": {},
   "source": [
    "# Load and normalize PubMedQA Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "\n",
    "def normalize_label(lbl: str) -> str:\n",
    "    lbl = (lbl or \"\").strip().lower()\n",
    "    if lbl in {\"yes\", \"y\", \"true\"}:\n",
    "        return \"yes\"\n",
    "    if lbl in {\"no\", \"n\", \"false\"}:\n",
    "        return \"no\"\n",
    "    return \"maybe\"\n",
    "\n",
    "def to_record(ex):\n",
    "    q = (ex.get(\"question\") or \"\").strip()\n",
    "\n",
    "    # handle context as string or list\n",
    "    cx = ex.get(\"context\") or ex.get(\"abstract\") or \"\"\n",
    "    if isinstance(cx, list):\n",
    "        cx = \" \".join([str(x).strip() for x in cx])\n",
    "    else:\n",
    "        cx = str(cx).strip()\n",
    "\n",
    "    y = normalize_label(ex.get(\"final_decision\") or ex.get(\"label\") or \"\")\n",
    "    return {\"question\": q, \"context\": cx, \"label\": y}\n",
    "\n",
    "# load raw dataset\n",
    "ds_raw = load_dataset(CONFIG[\"DATASET_NAME\"], CONFIG[\"DATASET_CONFIG\"])\n",
    "\n",
    "# ensure we have at least train / test\n",
    "if \"train\" not in ds_raw:\n",
    "    raise ValueError(\"This dataset must provide a 'train' split or be splittable.\")\n",
    "\n",
    "if \"test\" in ds_raw:\n",
    "    ds_work = DatasetDict({k: ds_raw[k] for k in ds_raw.keys()})\n",
    "elif \"validation\" in ds_raw:\n",
    "    ds_work = DatasetDict(train=ds_raw[\"train\"], test=ds_raw[\"validation\"])\n",
    "else:\n",
    "    tmp = ds_raw[\"train\"].train_test_split(test_size=0.15, seed=CONFIG[\"SEED\"])\n",
    "    ds_work = DatasetDict(train=tmp[\"train\"], test=tmp[\"test\"])\n",
    "\n",
    "# normalize columns to a common schema\n",
    "ds = DatasetDict({\n",
    "    split: ds_work[split].map(to_record, remove_columns=ds_work[split].column_names)\n",
    "    for split in ds_work.keys()\n",
    "})\n",
    "\n",
    "if CONFIG.get(\"TRIAL_LIMIT_TRAIN\"):\n",
    "    n = min(len(ds[\"train\"]), int(CONFIG[\"TRIAL_LIMIT_TRAIN\"]))\n",
    "    ds[\"train\"] = ds[\"train\"].select(range(n))\n",
    "\n",
    "if CONFIG.get(\"TRIAL_LIMIT_TEST\"):\n",
    "    n = min(len(ds[\"test\"]), int(CONFIG[\"TRIAL_LIMIT_TEST\"]))\n",
    "    ds[\"test\"] = ds[\"test\"].select(range(n))\n",
    "\n",
    "# constrain labels to {yes,no,maybe}\n",
    "LABELS = [\"yes\", \"no\", \"maybe\"]\n",
    "for split in list(ds.keys()):\n",
    "    ds[split] = ds[split].filter(lambda ex: ex[\"label\"] in LABELS)\n",
    "\n",
    "# basic checks and summary\n",
    "for split in [\"train\", \"test\"]:\n",
    "    if split not in ds:\n",
    "        raise ValueError(f\"Required split '{split}' is missing after preparation.\")\n",
    "\n",
    "print(\"splits available:\", list(ds.keys()))\n",
    "print(\"train size:\", len(ds[\"train\"]))\n",
    "print(\"test size:\", len(ds[\"test\"]))\n",
    "if \"validation\" in ds:\n",
    "    print(\"validation size:\", len(ds[\"validation\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29367ba5",
   "metadata": {},
   "source": [
    "#  Tokenizer setup \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"PROMPT_MODEL\"],\n",
    "    token=HF_TOKEN if HF_TOKEN else None\n",
    ")\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"tokenizer loaded:\", CONFIG[\"PROMPT_MODEL\"])\n",
    "print(\"pad_token:\", tokenizer.pad_token, \"id:\", tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)  \n",
    "print(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302b44d",
   "metadata": {},
   "source": [
    "# Prompt builders for different methods \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_prompt(question: str, context: str, style: str = \"domain\") -> str:\n",
    "    \"\"\"\n",
    "    Build a prompt for PubMedQA depending on the chosen style.\n",
    "    style options: \"zero\", \"domain\", \"cot\"\n",
    "    \"\"\"\n",
    "    if style == \"zero\":\n",
    "        return (\n",
    "            \"You are a helpful biomedical QA assistant .\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Context: {context}\\n\"\n",
    "            \"Answer with exactly one word: yes, no, or maybe.\\nAnswer:\"\n",
    "        )\n",
    "    elif style == \"cot\":\n",
    "        return (\n",
    "            \"You are a biomedical QA assistant. Think step by step.\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Context: {context}\\n\"\n",
    "            \"First reason step-by-step, then on a new line output: \"\n",
    "            \"Final answer: yes or no or maybe.\\nReasoning:\"\n",
    "        )\n",
    "    else:  # domain-specific\n",
    "        return (\n",
    "            \"You are a biomedical QA assistant specialized in PubMed abstracts.\\n\"\n",
    "            \"Use the context to answer exactly one word among: yes, no, maybe.\\n\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Context: {context}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "print(\"prompt builder ready. available styles:\", CONFIG[\"PROMPT_STYLES\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f043c",
   "metadata": {},
   "source": [
    "#  Label scoring utilities \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "902b4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "# fixed label set\n",
    "LABELS = [\"yes\", \"no\", \"maybe\"]\n",
    "\n",
    "# use a leading space for LLaMA tokenization\n",
    "VERBALIZERS = OrderedDict({\n",
    "    \"yes\":   \" yes\",\n",
    "    \"no\":    \" no\",\n",
    "    \"maybe\": \" maybe\",\n",
    "})\n",
    "\n",
    "def _first_real_device(model: torch.nn.Module) -> torch.device:\n",
    "    for p in model.parameters():\n",
    "        if getattr(p, \"device\", None) and p.device.type != \"meta\":\n",
    "            return p.device\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def _label_id_seqs(tokenizer) -> dict[str, torch.Tensor]:\n",
    "    seqs = {}\n",
    "    for lab, text in VERBALIZERS.items():\n",
    "        ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "        if not ids:\n",
    "            raise ValueError(f\"Tokenizer returned no ids for verbalizer: {text!r}\")\n",
    "        seqs[lab] = ids\n",
    "    return seqs\n",
    "\n",
    "@torch.no_grad()\n",
    "def _score_label_seq(model, prompt_ids: torch.Tensor, label_ids: torch.Tensor) -> float:\n",
    "    \"\"\"sum log P(y_1..y_K | prompt); avoids generation collapse to 'yes'.\"\"\"\n",
    "    total = 0.0\n",
    "    ctx = prompt_ids\n",
    "    for tok in label_ids:\n",
    "        out = model(input_ids=ctx.unsqueeze(0), attention_mask=torch.ones_like(ctx).unsqueeze(0))\n",
    "        logits = out.logits[:, -1, :]\n",
    "        logp = F.log_softmax(logits, dim=-1)[0, tok]\n",
    "        total += float(logp)\n",
    "        ctx = torch.cat([ctx, torch.tensor([tok], device=ctx.device, dtype=torch.long)], dim=0)\n",
    "    return total\n",
    "\n",
    "def predict_with_scoring(model, tokenizer, dataset, style: str, max_samples: int | None, max_input_tokens: int):\n",
    "    \"\"\"returns (preds, refs) using log-likelihood scoring over {yes,no,maybe}.\"\"\"\n",
    "    device = _first_real_device(model)\n",
    "    model.eval()\n",
    "    lab_seqs = {k: torch.tensor(v, device=device, dtype=torch.long) for k, v in _label_id_seqs(tokenizer).items()}\n",
    "    preds, refs = [], []\n",
    "    n = len(dataset) if not max_samples else min(len(dataset), int(max_samples))\n",
    "\n",
    "    for i in range(n):\n",
    "        ex = dataset[i]\n",
    "        prompt = build_prompt(ex[\"question\"], ex[\"context\"], style=style)\n",
    "        enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens, add_special_tokens=True)\n",
    "        input_ids = enc[\"input_ids\"][0].to(device)\n",
    "\n",
    "        scores = {lab: _score_label_seq(model, input_ids, lab_seqs[lab]) for lab in LABELS}\n",
    "        pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
    "\n",
    "        preds.append(pred)\n",
    "        refs.append(ex[\"label\"])\n",
    "    return preds, refs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df1448d",
   "metadata": {},
   "source": [
    "# QLoRA setup \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# bitsandbytes 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if CONFIG[\"BF16\"] else torch.float16,\n",
    ")\n",
    "\n",
    "lora_model = None\n",
    "tokenized_ds = None\n",
    "\n",
    "if CONFIG[\"USE_QLORA\"]:\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"QLoRA requires CUDA. Set USE_QLORA=False for CPU-only runs.\")\n",
    "    gpu_id = torch.cuda.current_device()\n",
    "    single_gpu_map = {\"\": gpu_id}\n",
    "\n",
    "    # load base model in 4-bit on the current GPU\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        CONFIG[\"LORA_BASE_MODEL\"],\n",
    "        device_map=single_gpu_map,                 \n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16 if CONFIG[\"BF16\"] else torch.float16,\n",
    "        token=HF_TOKEN if 'HF_TOKEN' in globals() and HF_TOKEN else None,\n",
    "    )\n",
    "\n",
    "    base.config.use_cache = False\n",
    "    base = prepare_model_for_kbit_training(base, use_gradient_checkpointing=True)\n",
    "\n",
    "    # lora configuration\n",
    "    lconf = LoraConfig(\n",
    "        r=CONFIG[\"QLORA_RANKS\"][0],\n",
    "        lora_alpha=CONFIG[\"LORA_ALPHA\"],\n",
    "        target_modules=CONFIG[\"TARGET_MODULES\"],\n",
    "        lora_dropout=CONFIG[\"LORA_DROPOUT\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    lora_model = get_peft_model(base, lconf)\n",
    "    print(\"LoRA model ready. rank:\", CONFIG[\"QLORA_RANKS\"][0], \"on GPU\", gpu_id)\n",
    "\n",
    "\n",
    "    # we’ll train the model to predict ONLY the final label token\n",
    "    LABELS = [\"yes\", \"no\", \"maybe\"]\n",
    "\n",
    "    def fmt_supervised(ex):\n",
    "        p = build_prompt(ex[\"question\"], ex[\"context\"], style=CONFIG.get(\"PROMPT_STYLE\", \"domain\"))\n",
    "        y = ex[\"label\"].strip().lower()\n",
    "        if y not in LABELS:\n",
    "            y = \"maybe\"\n",
    "        # add a single space so the next token is the label token\n",
    "        return {\"text\": f\"{p} {y}\"}\n",
    "\n",
    "    ds_lora = ds.map(fmt_supervised)\n",
    "\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        print(\"Tokenizer pad_token set to eos_token.\")\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    # tokenization with labels masked to only the last non-pad token\n",
    "    def tok_fn(batch):\n",
    "        enc = tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=CONFIG[\"MAX_INPUT_TOKENS\"],\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        labels = []\n",
    "        for ids in input_ids:\n",
    "            last_idx = len(ids) - 1\n",
    "            while last_idx >= 0 and ids[last_idx] == pad_id:\n",
    "                last_idx -= 1\n",
    "            lab = [-100] * len(ids)\n",
    "            if last_idx >= 0:\n",
    "                lab[last_idx] = ids[last_idx]\n",
    "            labels.append(lab)\n",
    "        enc[\"labels\"] = labels\n",
    "        return enc\n",
    "\n",
    "    tokenized_ds = ds_lora.map(\n",
    "        tok_fn,\n",
    "        batched=True,\n",
    "        remove_columns=ds_lora[\"train\"].column_names\n",
    "    )\n",
    "    print(\"Tokenized dataset:\", tokenized_ds)\n",
    "else:\n",
    "    print(\"USE_QLORA is False. Skipping LoRA setup and tokenization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e268eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62908608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch\n",
    "START_TIME = time.time()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab3792",
   "metadata": {},
   "source": [
    "# LoRA training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "train_output = None\n",
    "\n",
    "lora_ready = (\n",
    "    CONFIG.get(\"USE_QLORA\", False)\n",
    "    and (\"lora_model\" in globals()) and (lora_model is not None)\n",
    "    and (\"tokenized_ds\" in globals()) and (tokenized_ds is not None)\n",
    "    and (\"train\" in tokenized_ds)\n",
    ")\n",
    "\n",
    "if not lora_ready:\n",
    "    print(\"LoRA training skipped (USE_QLORA=False or LoRA setup/tokenized data missing).\")\n",
    "else:\n",
    "\n",
    "    train_ds = tokenized_ds[\"train\"]\n",
    "    if CONFIG.get(\"TRIAL_LIMIT_TRAIN\"):\n",
    "        n = min(len(train_ds), int(CONFIG[\"TRIAL_LIMIT_TRAIN\"]))\n",
    "        train_ds = train_ds.select(range(n))\n",
    "        print(f\"using trial cap for training: {n} examples\")\n",
    "\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=os.path.join(CONFIG[\"WORK_DIR\"], \"lora\"),\n",
    "        num_train_epochs=CONFIG[\"EPOCHS\"],\n",
    "        per_device_train_batch_size=CONFIG[\"PER_DEVICE_TRAIN_BS\"],\n",
    "        gradient_accumulation_steps=CONFIG[\"GRAD_ACCUM_STEPS\"],\n",
    "        learning_rate=CONFIG[\"LR\"],\n",
    "        warmup_ratio=CONFIG[\"WARMUP_RATIO\"],\n",
    "        logging_dir=os.path.join(CONFIG[\"WORK_DIR\"], \"logs\"),\n",
    "        logging_steps=50,\n",
    "        bf16=bool(CONFIG.get(\"BF16\", False)),\n",
    "        report_to=CONFIG.get(\"REPORT_TO\", \"none\"),\n",
    "        save_total_limit=2,\n",
    "        gradient_checkpointing=True,\n",
    "        eval_strategy=\"no\",        \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    train_output = trainer.train()\n",
    "    print(\"training finished\")\n",
    "    print(\"train metrics:\", getattr(train_output, \"metrics\", {}))\n",
    "\n",
    "    save_dir = os.path.join(CONFIG[\"WORK_DIR\"], \"lora_final\")\n",
    "    trainer.save_model(save_dir)\n",
    "    print(\"saved:\", save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a3169",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- helpers ---\n",
    "def now_str():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True); return p\n",
    "\n",
    "def safe_float(x, default=0.0):\n",
    "    try:\n",
    "        if x is None: return default\n",
    "        xf = float(x)\n",
    "        if math.isnan(xf) or math.isinf(xf): return default\n",
    "        return xf\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "# prompt builder fallback (if not defined earlier)\n",
    "if \"build_prompt\" not in globals():\n",
    "    def build_prompt(question, context, style=\"domain\"):\n",
    "        if style == \"zero\":\n",
    "            return (\n",
    "                \"You are a helpful biomedical QA assistant.\\n\"\n",
    "                f\"Question: {question}\\nContext: {context}\\n\"\n",
    "                \"Answer with exactly one word: yes, no, or maybe.\\nAnswer:\"\n",
    "            )\n",
    "        if style == \"cot\":\n",
    "            return (\n",
    "                \"You are a biomedical QA assistant. Think step by step.\\n\"\n",
    "                f\"Question: {question}\\nContext: {context}\\n\"\n",
    "                \"First reason step-by-step, then on a new line output: Final answer: yes or no or maybe.\\nReasoning:\"\n",
    "            )\n",
    "        return (\n",
    "            \"You are a biomedical QA assistant specialized in PubMed abstracts.\\n\"\n",
    "            \"Use the context to answer exactly one word among: yes, no, maybe.\\n\\n\"\n",
    "            f\"Question: {question}\\nContext: {context}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "\n",
    "def score_labels_for_prompt(model, tokenizer, prompt_text, labels, device, max_len):\n",
    "    \"\"\"\n",
    "    computes summed log-prob of each candidate label given the prompt.\n",
    "    picks argmax over labels.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_ids = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=max_len)[\"input_ids\"][0]\n",
    "    best_label, best_lp = None, -1e30\n",
    "    with torch.no_grad():\n",
    "        for lab in labels:\n",
    "            lab_text = \" \" + lab  # space helps produce the expected first token\n",
    "            ids_full = tokenizer(prompt_text + lab_text, return_tensors=\"pt\").to(device)[\"input_ids\"][0]\n",
    "\n",
    "            # figure out where the label tokens start in the full sequence\n",
    "            ids_lab = tokenizer(lab_text, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "\n",
    "            # if truncated, ensure we only score the tail that exists\n",
    "            if len(ids_full) < len(ids_lab) + 1:\n",
    "                \n",
    "                lp = -1e30\n",
    "\n",
    "            else:\n",
    "                # forward pass on full\n",
    "                out = model(input_ids=ids_full.unsqueeze(0).to(device))\n",
    "                logits = out.logits[0]  # [seq, vocab]\n",
    "\n",
    "                # positions corresponding to label tokens are the last len(ids_lab) steps\n",
    "                start = logits.size(0) - len(ids_lab)\n",
    "\n",
    "                # sum log-softmax probs of true label tokens at those positions\n",
    "                logprobs = torch.log_softmax(logits[start-1:start-1+len(ids_lab)], dim=-1)\n",
    "\n",
    "                # pick the probability assigned to the correct next token at each step\n",
    "                token_lp = logprobs[torch.arange(len(ids_lab)), ids_lab.to(device)]\n",
    "                lp = float(token_lp.sum().item())\n",
    "            if lp > best_lp:\n",
    "                best_lp, best_label = lp, lab\n",
    "    return best_label, best_lp\n",
    "\n",
    "# --- paths and run id ---\n",
    "WORK_DIR = CONFIG[\"WORK_DIR\"]\n",
    "LOG_JSONL = os.path.join(WORK_DIR, \"experiment_log.jsonl\")\n",
    "LEADERBOARD_CSV = os.path.join(WORK_DIR, \"experiment_leaderboard.csv\")\n",
    "RUN_ID = os.environ.get(\"RUN_ID\", str(int(time.time())))\n",
    "RUN_DIR = ensure_dir(os.path.join(WORK_DIR, \"artifacts\", RUN_ID))\n",
    "\n",
    "LABELS = [\"yes\", \"no\", \"maybe\"]\n",
    "use_lora = bool(CONFIG.get(\"USE_QLORA\", False))\n",
    "\n",
    "\n",
    "if \"ds\" not in globals() or \"test\" not in ds:\n",
    "    raise RuntimeError(\"Test split not found. Ensure the data loading cell created `ds['test']`.\")\n",
    "\n",
    "test_set = ds[\"test\"]\n",
    "\n",
    "# -------------------------\n",
    "# path A: LoRA evaluation\n",
    "# -------------------------\n",
    "if use_lora:\n",
    "    if \"lora_model\" not in globals() or lora_model is None:\n",
    "        raise RuntimeError(\"LoRA model not found in memory. Train it first or set USE_QLORA=False for prompt baselines.\")\n",
    "\n",
    "    lora_model.eval()\n",
    "    first_device = next(p.device for p in lora_model.parameters() if p.device.type != \"meta\")\n",
    "    print(f\"[LoRA] evaluating on device: {first_device}\")\n",
    "\n",
    "    preds, refs = [], []\n",
    "    for ex in test_set:\n",
    "        prompt = build_prompt(ex[\"question\"], ex[\"context\"], style=\"domain\")\n",
    "        # use the same scoring approach for consistency\n",
    "        pred, _ = score_labels_for_prompt(\n",
    "            lora_model, tokenizer, prompt, LABELS, first_device, CONFIG[\"MAX_INPUT_TOKENS\"]\n",
    "        )\n",
    "        preds.append(pred)\n",
    "        refs.append(ex[\"label\"])\n",
    "\n",
    "    # reports\n",
    "    report = classification_report(refs, preds, labels=LABELS, output_dict=True, zero_division=0)\n",
    "    print(\"\\n=== LoRA Test Report ===\")\n",
    "    print(classification_report(refs, preds, labels=LABELS, zero_division=0))\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(refs, preds, labels=LABELS)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABELS)\n",
    "    disp.plot(cmap=\"Purples\")\n",
    "    plt.title(\"LoRA — Test Confusion Matrix\")\n",
    "    plt.savefig(os.path.join(RUN_DIR, \"lora_test_confusion_matrix.png\"), bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    train_loss = safe_float(train_output.metrics.get(\"train_loss\") if \"train_output\" in globals() else None)\n",
    "    train_rt   = safe_float(train_output.metrics.get(\"train_runtime\") if \"train_output\" in globals() else None)\n",
    "    train_sps  = safe_float(train_output.metrics.get(\"train_samples_per_second\") if \"train_output\" in globals() else None)\n",
    "\n",
    "    summary = {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"timestamp\": now_str(),\n",
    "        \"mode\": \"qlora\",\n",
    "        \"params\": {\n",
    "            \"MODEL\": CONFIG[\"LORA_BASE_MODEL\"],\n",
    "            \"QLORA_RANK\": CONFIG[\"QLORA_RANKS\"][0],\n",
    "            \"MAX_INPUT_TOKENS\": CONFIG[\"MAX_INPUT_TOKENS\"],\n",
    "            \"LR\": CONFIG[\"LR\"],\n",
    "            \"EPOCHS\": CONFIG[\"EPOCHS\"],\n",
    "            \"GRAD_ACCUM_STEPS\": CONFIG[\"GRAD_ACCUM_STEPS\"],\n",
    "            \"PER_DEVICE_TRAIN_BS\": CONFIG[\"PER_DEVICE_TRAIN_BS\"],\n",
    "            \"PROMPT_STYLE\": \"domain\",\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"test_accuracy\": safe_float(report.get(\"accuracy\")),\n",
    "            \"test_macro_f1\": safe_float(report.get(\"macro avg\", {}).get(\"f1-score\")),\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_runtime\": train_rt,\n",
    "            \"train_samples_per_second\": train_sps,\n",
    "        },\n",
    "        \"artifacts\": {\n",
    "            \"test_confusion_matrix\": os.path.join(RUN_DIR, \"lora_test_confusion_matrix.png\"),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # write logs\n",
    "    with open(LOG_JSONL, \"a\") as f:\n",
    "        f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "    row = {\n",
    "        \"run_id\": summary[\"run_id\"],\n",
    "        \"timestamp\": summary[\"timestamp\"],\n",
    "        \"mode\": summary[\"mode\"],\n",
    "        \"model\": summary[\"params\"][\"MODEL\"],\n",
    "        \"rank\": summary[\"params\"][\"QLORA_RANK\"],\n",
    "        \"epochs\": summary[\"params\"][\"EPOCHS\"],\n",
    "        \"lr\": summary[\"params\"][\"LR\"],\n",
    "        \"val_acc\": None,  \n",
    "        \"val_f1\":  None,  \n",
    "        \"test_acc\": summary[\"metrics\"][\"test_accuracy\"],\n",
    "        \"test_f1\": summary[\"metrics\"][\"test_macro_f1\"],\n",
    "    }\n",
    "    if os.path.exists(LEADERBOARD_CSV):\n",
    "        df_lb = pd.read_csv(LEADERBOARD_CSV)\n",
    "        missing = [c for c in row.keys() if c not in df_lb.columns]\n",
    "        for c in missing: df_lb[c] = pd.NA\n",
    "        df_lb = df_lb[list(row.keys())]\n",
    "    else:\n",
    "        df_lb = pd.DataFrame(columns=list(row.keys()))\n",
    "    df_lb = pd.concat([df_lb, pd.DataFrame([row])], ignore_index=True)\n",
    "    df_lb.to_csv(LEADERBOARD_CSV, index=False)\n",
    "\n",
    "    print(f\"logged (JSONL): {LOG_JSONL}\")\n",
    "    print(f\"leaderboard updated: {LEADERBOARD_CSV}\")\n",
    "    print(f\"artifacts saved to: {RUN_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# path B: Prompt baselines only\n",
    "# -----------------------------\n",
    "else:\n",
    "\n",
    "    prompt_model_id = CONFIG[\"PROMPT_MODEL\"]\n",
    "    prompt_model = AutoModelForCausalLM.from_pretrained(\n",
    "        prompt_model_id,\n",
    "        device_map=\"auto\",\n",
    "        #device_map={\"\":1},\n",
    "        torch_dtype=torch.bfloat16 if CONFIG.get(\"BF16\", False) else torch.float16,\n",
    "        token=HF_TOKEN if \"HF_TOKEN\" in globals() and HF_TOKEN else None\n",
    "    )\n",
    "    prompt_model.eval()\n",
    "    first_device = next(p.device for p in prompt_model.parameters() if p.device.type != \"meta\")\n",
    "    print(f\"[Prompt] evaluating on device: {first_device}\")\n",
    "\n",
    "\n",
    "    for style in CONFIG.get(\"PROMPT_STYLES\", [\"zero\",\"domain\",\"cot\"]):\n",
    "\n",
    "        preds, refs = [], []\n",
    "        for ex in test_set:\n",
    "            prompt = build_prompt(ex[\"question\"], ex[\"context\"], style=style)\n",
    "\n",
    "            # label scoring (no generation)\n",
    "            pred, _ = score_labels_for_prompt(\n",
    "                prompt_model, tokenizer, prompt, LABELS, first_device, CONFIG[\"MAX_INPUT_TOKENS\"]\n",
    "            )\n",
    "            preds.append(pred)\n",
    "            refs.append(ex[\"label\"])\n",
    "\n",
    "\n",
    "        report = classification_report(refs, preds, labels=LABELS, output_dict=True, zero_division=0)\n",
    "        print(f\"\\n=== Prompt Test Report — {style} ===\")\n",
    "        print(classification_report(refs, preds, labels=LABELS, zero_division=0))\n",
    "\n",
    "\n",
    "        cm = confusion_matrix(refs, preds, labels=LABELS)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABELS)\n",
    "        disp.plot(cmap=\"Blues\")\n",
    "        plt.title(f\"Prompt ({style}) — Test Confusion Matrix\")\n",
    "        fig_path = os.path.join(RUN_DIR, f\"prompt_{style}_test_confusion_matrix.png\")\n",
    "        plt.savefig(fig_path, bbox_inches=\"tight\", dpi=150)\n",
    "        plt.show()\n",
    "\n",
    "        summary = {\n",
    "            \"run_id\": RUN_ID,\n",
    "            \"timestamp\": now_str(),\n",
    "            \"mode\": f\"prompt/{style}\",\n",
    "            \"params\": {\n",
    "                \"MODEL\": prompt_model_id,\n",
    "                \"PROMPT_STYLE\": style,\n",
    "                \"MAX_INPUT_TOKENS\": CONFIG[\"MAX_INPUT_TOKENS\"],\n",
    "                \"MAX_NEW_TOKENS\": CONFIG[\"MAX_NEW_TOKENS\"],\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"test_accuracy\": safe_float(report.get(\"accuracy\")),\n",
    "                \"test_macro_f1\": safe_float(report.get(\"macro avg\", {}).get(\"f1-score\")),\n",
    "            },\n",
    "            \"artifacts\": {\n",
    "                \"test_confusion_matrix\": fig_path,\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(LOG_JSONL, \"a\") as f:\n",
    "            f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "        row = {\n",
    "            \"run_id\": summary[\"run_id\"],\n",
    "            \"timestamp\": summary[\"timestamp\"],\n",
    "            \"mode\": summary[\"mode\"],\n",
    "            \"model\": summary[\"params\"][\"MODEL\"],\n",
    "            \"rank\": None,     \n",
    "            \"epochs\": None,   \n",
    "            \"lr\": None,       \n",
    "            \"val_acc\": None,  \n",
    "            \"val_f1\": None,   \n",
    "            \"test_acc\": summary[\"metrics\"][\"test_accuracy\"],\n",
    "            \"test_f1\": summary[\"metrics\"][\"test_macro_f1\"],\n",
    "        }\n",
    "        if os.path.exists(LEADERBOARD_CSV):\n",
    "            df_lb = pd.read_csv(LEADERBOARD_CSV)\n",
    "            missing = [c for c in row.keys() if c not in df_lb.columns]\n",
    "            for c in missing: df_lb[c] = pd.NA\n",
    "            df_lb = df_lb[list(row.keys())]\n",
    "        else:\n",
    "            df_lb = pd.DataFrame(columns=list(row.keys()))\n",
    "        df_lb = pd.concat([df_lb, pd.DataFrame([row])], ignore_index=True)\n",
    "        df_lb.to_csv(LEADERBOARD_CSV, index=False)\n",
    "\n",
    "        print(f\"[{style}] logged (JSONL): {LOG_JSONL}\")\n",
    "        print(f\"[{style}] leaderboard updated: {LEADERBOARD_CSV}\")\n",
    "\n",
    "    print(f\"artifacts saved to: {RUN_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdbb239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -f \"{CONFIG['WORK_DIR']}/experiment_log.jsonl\" \"{CONFIG['WORK_DIR']}/experiment_leaderboard.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820d461",
   "metadata": {},
   "source": [
    "# Results and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837da312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "WORK_DIR = Path(CONFIG[\"WORK_DIR\"])\n",
    "LOG_JSONL = WORK_DIR / \"experiment_log.jsonl\"\n",
    "LEADERBOARD_CSV = WORK_DIR / \"experiment_leaderboard.csv\"\n",
    "\n",
    "def load_csv_as_df(path: Path) -> pd.DataFrame:\n",
    "    if path.exists():\n",
    "        try:\n",
    "            return pd.read_csv(path)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def coerce_numeric(df: pd.DataFrame, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df_csv = load_csv_as_df(LEADERBOARD_CSV)\n",
    "print(f\"Loaded CSV rows: {len(df_csv)}\")\n",
    "\n",
    "if not df_csv.empty:\n",
    "\n",
    "    drop_cols = [c for c in [\"val_acc\", \"val_f1\"] if c in df_csv.columns]\n",
    "    dfc = df_csv.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "\n",
    "    dfc = coerce_numeric(dfc, [\"rank\", \"epochs\", \"lr\", \"test_acc\", \"test_f1\"])\n",
    "\n",
    "    # show top runs sorted by test_acc (fallback test_f1)\n",
    "    sort_col = \"test_acc\" if \"test_acc\" in dfc.columns else (\"test_f1\" if \"test_f1\" in dfc.columns else None)\n",
    "    if sort_col:\n",
    "        display(dfc.sort_values(sort_col, ascending=False).head(10))\n",
    "    else:\n",
    "        display(dfc.head(10))\n",
    "\n",
    "    # best-per-mode summary\n",
    "    if \"mode\" in dfc.columns and \"test_acc\" in dfc.columns:\n",
    "        dfm = dfc.dropna(subset=[\"test_acc\", \"mode\"])\n",
    "        if not dfm.empty:\n",
    "            idx = dfm.groupby(\"mode\")[\"test_acc\"].idxmax()\n",
    "            best_per_mode = dfm.loc[idx, [\"mode\", \"timestamp\", \"run_id\", \"model\", \"test_acc\", \"test_f1\"]] \\\n",
    "                               .sort_values(\"test_acc\", ascending=False)\n",
    "            print(\"\\nBest test metrics per mode (from CSV):\")\n",
    "            display(best_per_mode)\n",
    "\n",
    "            # bar plot\n",
    "            plt.figure()\n",
    "            best_per_mode.set_index(\"mode\")[[\"test_acc\"]].plot(kind=\"bar\", legend=False, ax=plt.gca())\n",
    "            plt.ylabel(\"Test accuracy\")\n",
    "            plt.title(\"Best per mode (higher is better)\")\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No leaderboard CSV yet.\")\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<hr>\n",
    "<p><b>Logged files:</b></p>\n",
    "<ul>\n",
    "  <li><a href=\"{LOG_JSONL}\" target=\"_blank\">JSONL log file</a></li>\n",
    "  <li><a href=\"{LEADERBOARD_CSV}\" target=\"_blank\">Leaderboard CSV</a></li>\n",
    "</ul>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d769f9",
   "metadata": {},
   "source": [
    "# Log GPU Memory Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e5fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "WORK_DIR = CONFIG[\"WORK_DIR\"]\n",
    "LOG_JSONL = os.path.join(WORK_DIR, \"experiment_log.jsonl\")\n",
    "LEADERBOARD_CSV = os.path.join(WORK_DIR, \"experiment_leaderboard.csv\")\n",
    "\n",
    "def now_str():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def gpu_stats_dict():\n",
    "    if not torch.cuda.is_available():\n",
    "        return {}\n",
    "    return {\n",
    "        \"gpu_index\": torch.cuda.current_device(),\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0),\n",
    "        \"mem_allocated_mb\": round(torch.cuda.memory_allocated() / 1024**2, 2),\n",
    "        \"mem_reserved_mb\": round(torch.cuda.memory_reserved() / 1024**2, 2),\n",
    "        \"max_mem_allocated_mb\": round(torch.cuda.max_memory_allocated() / 1024**2, 2),\n",
    "        \"max_mem_reserved_mb\": round(torch.cuda.max_memory_reserved() / 1024**2, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "run_id    = RUN_ID if \"RUN_ID\" in globals() else str(int(time.time()))\n",
    "mode      = \"qlora\" if CONFIG.get(\"USE_QLORA\") else f\"prompt/{CONFIG.get('PROMPT_STYLES', ['zero'])[0]}\"\n",
    "model_id  = CONFIG[\"LORA_BASE_MODEL\"] if CONFIG.get(\"USE_QLORA\") else CONFIG[\"PROMPT_MODEL\"]\n",
    "runtime_s = round(time.time() - START_TIME, 2) if \"START_TIME\" in globals() else None\n",
    "gpu_info  = gpu_stats_dict()\n",
    "\n",
    "util_entry = {\n",
    "    \"run_id\": run_id,\n",
    "    \"timestamp\": now_str(),\n",
    "    \"mode\": mode,\n",
    "    \"model\": model_id,\n",
    "    \"runtime_sec\": runtime_s,\n",
    "    \"gpu\": gpu_info,\n",
    "}\n",
    "\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "with open(LOG_JSONL, \"a\") as f:\n",
    "    f.write(json.dumps(util_entry) + \"\\n\")\n",
    "\n",
    "util_cols = [\"runtime_sec\", \"gpu_mem_max_mb\", \"gpu_name\"]\n",
    "row_update = {\n",
    "    \"run_id\": run_id,\n",
    "    \"runtime_sec\": runtime_s,\n",
    "    \"gpu_mem_max_mb\": gpu_info.get(\"max_mem_allocated_mb\", None) if gpu_info else None,\n",
    "    \"gpu_name\": gpu_info.get(\"gpu_name\", None) if gpu_info else None,\n",
    "}\n",
    "\n",
    "if os.path.exists(LEADERBOARD_CSV):\n",
    "    df = pd.read_csv(LEADERBOARD_CSV)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\n",
    "        \"run_id\",\"timestamp\",\"mode\",\"model\",\"rank\",\"epochs\",\"lr\",\n",
    "        \"test_acc\",\"test_f1\",\"runtime_sec\",\"gpu_mem_max_mb\",\"gpu_name\",\"train_loss\"\n",
    "    ])\n",
    "\n",
    "for c in row_update.keys():\n",
    "    if c not in df.columns:\n",
    "        df[c] = pd.NA\n",
    "\n",
    "idx = df.index[df[\"run_id\"] == run_id].tolist()\n",
    "if idx:\n",
    "    i = idx[0]\n",
    "    for k, v in row_update.items():\n",
    "        df.at[i, k] = v\n",
    "else:\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame([row_update])], ignore_index=True)\n",
    "\n",
    "df.to_csv(LEADERBOARD_CSV, index=False)\n",
    "\n",
    "print(\"=== Utilization captured ===\")\n",
    "print(json.dumps(util_entry, indent=2))\n",
    "print(f\"Appended to JSONL: {LOG_JSONL}\")\n",
    "print(f\"Upserted in CSV:   {LEADERBOARD_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f0b0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Merge duplicate entries for a run_id \n",
    "# import os, json\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# WORK_DIR = Path(CONFIG[\"WORK_DIR\"])\n",
    "# CSV_PATH = WORK_DIR / \"experiment_leaderboard.csv\"\n",
    "# JSONL_PATH = WORK_DIR / \"experiment_log.jsonl\"\n",
    "\n",
    "# # map JSONL keys to CSV columns\n",
    "# JSONL_TO_CSV = {\n",
    "#     \"timestamp\": \"timestamp\",\n",
    "#     \"mode\": \"mode\",\n",
    "#     \"params.MODEL\": \"model\",\n",
    "#     \"params.QLORA_RANK\": \"rank\",\n",
    "#     \"params.EPOCHS\": \"epochs\",\n",
    "#     \"params.LR\": \"lr\",\n",
    "#     \"metrics.val_accuracy\": \"val_acc\",\n",
    "#     \"metrics.val_macro_f1\": \"val_f1\",\n",
    "#     \"metrics.test_accuracy\": \"test_acc\",\n",
    "#     \"metrics.test_macro_f1\": \"test_f1\",\n",
    "#     \"metrics.train_loss\": \"train_loss\",\n",
    "#     \"metrics.train_runtime\": \"runtime_sec\",\n",
    "#     \"env.gpu.gpu_name\": \"gpu_name\",\n",
    "#     \"env.gpu.max_mem_allocated_mb\": \"gpu_mem_max_mb\",\n",
    "# }\n",
    "\n",
    "# NUMERIC_COLS = [\n",
    "#     \"rank\",\"epochs\",\"lr\",\"val_acc\",\"val_f1\",\"test_acc\",\"test_f1\",\n",
    "#     \"runtime_sec\",\"gpu_mem_max_mb\",\"train_loss\"\n",
    "# ]\n",
    "\n",
    "# def _norm_runid(x) -> str:\n",
    "#     \"\"\"normalize run_id to a clean string (handles ints, floats, strings).\"\"\"\n",
    "#     if pd.isna(x):\n",
    "#         return \"\"\n",
    "#     if isinstance(x, (int,)):\n",
    "#         return str(x)\n",
    "#     if isinstance(x, float):\n",
    "#         # handle floats serialized from CSV (e.g., 1.758476432e+09 or 1758476432.0)\n",
    "#         return str(int(x))\n",
    "#     s = str(x).strip()\n",
    "#     # common cases: \"1758476432.0\" or \"1.758476432e+09\"\n",
    "#     try:\n",
    "#         if \".\" in s or \"e+\" in s.lower() or \"e-\" in s.lower():\n",
    "#             return str(int(float(s)))\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     return s\n",
    "\n",
    "# def _load_csv(csv_path: Path) -> pd.DataFrame:\n",
    "#     if not csv_path.exists():\n",
    "#         return pd.DataFrame()\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     if \"run_id\" in df.columns:\n",
    "#         df[\"run_id\"] = df[\"run_id\"].apply(_norm_runid)\n",
    "#     for c in NUMERIC_COLS:\n",
    "#         if c in df.columns:\n",
    "#             df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "#     return df\n",
    "\n",
    "# def _load_jsonl(jsonl_path: Path) -> pd.DataFrame:\n",
    "#     rows = []\n",
    "#     if jsonl_path.exists():\n",
    "#         with open(jsonl_path, \"r\") as f:\n",
    "#             for line in f:\n",
    "#                 line = line.strip()\n",
    "#                 if not line:\n",
    "#                     continue\n",
    "#                 try:\n",
    "#                     rows.append(json.loads(line))\n",
    "#                 except json.JSONDecodeError:\n",
    "#                     continue\n",
    "#     if not rows:\n",
    "#         return pd.DataFrame()\n",
    "#     df = pd.json_normalize(rows)\n",
    "#     if \"run_id\" in df.columns:\n",
    "#         df[\"run_id\"] = df[\"run_id\"].apply(_norm_runid)\n",
    "#     # coerce mapped numeric columns if they exist\n",
    "#     for j_key, c_name in JSONL_TO_CSV.items():\n",
    "#         if j_key in df.columns and c_name in NUMERIC_COLS:\n",
    "#             df[j_key] = pd.to_numeric(df[j_key], errors=\"coerce\")\n",
    "#     return df\n",
    "\n",
    "# def _jsonl_rows_to_csv_schema(sub_jsonl: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"convert JSONL normalized rows to the CSV schema via mapping.\"\"\"\n",
    "#     if sub_jsonl.empty:\n",
    "#         return pd.DataFrame()\n",
    "#     out = pd.DataFrame(index=sub_jsonl.index)\n",
    "#     for j_key, c_name in JSONL_TO_CSV.items():\n",
    "#         if j_key in sub_jsonl.columns:\n",
    "#             out[c_name] = sub_jsonl[j_key]\n",
    "#     # ensure essential identifiers exist\n",
    "#     if \"run_id\" in sub_jsonl.columns:\n",
    "#         out[\"run_id\"] = sub_jsonl[\"run_id\"]\n",
    "#     return out\n",
    "\n",
    "# def _merge_rows(rows: pd.DataFrame) -> dict:\n",
    "#     \"\"\"merge multiple rows (same schema) by taking the first non-null per column.\"\"\"\n",
    "#     merged = {}\n",
    "#     for col in rows.columns:\n",
    "#         vals = rows[col].dropna()\n",
    "#         merged[col] = vals.iloc[0] if not vals.empty else pd.NA\n",
    "#     return merged\n",
    "\n",
    "# def merge_runid_entries(run_id: str, csv_path: Path = CSV_PATH, jsonl_path: Path = JSONL_PATH):\n",
    "#     rid = _norm_runid(run_id)\n",
    "\n",
    "#     df_csv = _load_csv(csv_path)\n",
    "#     df_jl  = _load_jsonl(jsonl_path)\n",
    "\n",
    "#     if df_csv.empty and df_jl.empty:\n",
    "#         print(\"No logs found in either CSV or JSONL.\")\n",
    "#         return\n",
    "\n",
    "#     # gather CSV rows\n",
    "#     sub_csv = df_csv[df_csv.get(\"run_id\", \"\").astype(str) == rid].copy()\n",
    "\n",
    "#     # gather JSONL rows and map to CSV schema\n",
    "#     sub_jl_raw = df_jl[df_jl.get(\"run_id\", \"\").astype(str) == rid].copy()\n",
    "#     sub_jl = _jsonl_rows_to_csv_schema(sub_jl_raw)\n",
    "\n",
    "#     if sub_csv.empty and sub_jl.empty:\n",
    "#         print(f\"No entries found for run_id={rid} in CSV or JSONL.\")\n",
    "#         # optional: debug available ids\n",
    "#         if not df_csv.empty:\n",
    "#             print(\"CSV run_ids sample:\", df_csv[\"run_id\"].astype(str).unique()[:10])\n",
    "#         if not df_jl.empty:\n",
    "#             print(\"JSONL run_ids sample:\", df_jl[\"run_id\"].astype(str).unique()[:10])\n",
    "#         return\n",
    "\n",
    "#     # if multiple modes exist for this run_id (e.g., prompt/zero, prompt/cot),\n",
    "#     # merge separately per mode to preserve comparability.\n",
    "#     modes_csv = set(sub_csv[\"mode\"].dropna().astype(str)) if \"mode\" in sub_csv.columns else set()\n",
    "#     modes_jl  = set(sub_jl[\"mode\"].dropna().astype(str))  if \"mode\" in sub_jl.columns  else set()\n",
    "#     modes = modes_csv.union(modes_jl) or {pd.NA}\n",
    "\n",
    "#     merged_rows = []\n",
    "#     for m in modes:\n",
    "#         # filter by mode (if available)\n",
    "#         part_csv = sub_csv if m is pd.NA else sub_csv[sub_csv.get(\"mode\").astype(str) == str(m)]\n",
    "#         part_jl  = sub_jl  if m is pd.NA else sub_jl[sub_jl.get(\"mode\").astype(str) == str(m)]\n",
    "\n",
    "#         # align columns for merge\n",
    "#         cols = sorted(set(part_csv.columns).union(part_jl.columns))\n",
    "#         part_csv = part_csv.reindex(columns=cols)\n",
    "#         part_jl  = part_jl.reindex(columns=cols)\n",
    "\n",
    "#         # stack all candidate rows with a simple priority: more complete rows first\n",
    "#         combined = pd.concat([part_csv, part_jl], ignore_index=True)\n",
    "#         if combined.empty:\n",
    "#             continue\n",
    "#         combined[\"__nnz__\"] = combined.notna().sum(axis=1)\n",
    "#         combined = combined.sort_values(\"__nnz__\", ascending=False).drop(columns=\"__nnz__\")\n",
    "\n",
    "#         merged = _merge_rows(combined)\n",
    "#         merged[\"run_id\"] = rid\n",
    "#         if m is not pd.NA:\n",
    "#             merged[\"mode\"] = m\n",
    "#         merged_rows.append(merged)\n",
    "\n",
    "#     if not merged_rows:\n",
    "#         print(f\"No mergeable rows for run_id={rid}.\")\n",
    "#         return\n",
    "\n",
    "#     # drop existing rows for this run_id (all modes) then append merged ones\n",
    "#     df_new = df_csv[df_csv.get(\"run_id\", \"\").astype(str) != rid].copy()\n",
    "#     df_new = pd.concat([df_new, pd.DataFrame(merged_rows)], ignore_index=True)\n",
    "\n",
    "#     # enforce column order (optional)\n",
    "#     preferred_cols = [\n",
    "#         \"run_id\",\"timestamp\",\"mode\",\"model\",\"rank\",\"epochs\",\"lr\",\n",
    "#         \"val_acc\",\"val_f1\",\"test_acc\",\"test_f1\",\n",
    "#         \"runtime_sec\",\"gpu_mem_max_mb\",\"gpu_name\",\"train_loss\"\n",
    "#     ]\n",
    "#     for c in preferred_cols:\n",
    "#         if c not in df_new.columns:\n",
    "#             df_new[c] = pd.NA\n",
    "#     df_new = df_new[preferred_cols + [c for c in df_new.columns if c not in preferred_cols]]\n",
    "\n",
    "#     # numeric coercion\n",
    "#     for c in NUMERIC_COLS:\n",
    "#         if c in df_new.columns:\n",
    "#             df_new[c] = pd.to_numeric(df_new[c], errors=\"coerce\")\n",
    "\n",
    "#     df_new.to_csv(csv_path, index=False)\n",
    "\n",
    "#     print(f\"Merged duplicates for run_id={rid}.\")\n",
    "#     display(df_new[df_new[\"run_id\"].astype(str) == rid])\n",
    "\n",
    "# # --- usage ---\n",
    "# merge_runid_entries(\"1758476432\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
